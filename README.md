# ðŸ”¥ Ember

Ember is a lightweight statistics and ML library for my personal use with C++ and Python. 

- [Installation](#installation) 
- [Getting Started](#getting-started)
  - [Ember Tensors](#ember-tensors)
  - [Datasets](#datasets)
  - [Models and Training](#models-and-training)
  - [Monte Carlo Samplers](#monte-carlo-samplers)

Look [here](docs/progress.md) to see the methods it supports. 

## Installation

  ### Compiling the `aten` Library  

  Git clone the repo, then pip install editable, which will run `setup.py`. I've painstakingly modified the Makefiles to make the installation as portable and easy as possible. You need Python 3.12 for now. 

  ```git clone git@github.com:mbahng/pyember.git 
  cd pyember 
  pip install -e .
  ```

  This runs `cmake` on `CMakeLists.txt`, which calls `aten/CMakeLists.txt` that compiles and links the source files in the C++ tensor library, which in turn (by default argument `BUILD_PYTHON_BINDINGS=ON`) calls `aten/bindings/CMakeLists.txt` to further generate a `.so` file that can be imported into `ember`. If there are problems, you should first check 

  1. Whether `build/` has been created. This is the first step in `setup.py` 
  2. Whether the compiled `main.cpp` and C++ unit test files have been compiled, i.e. if `aten/main` and `aten/tests` executables exist. 
  3. Whether `build/lib*/ember/aten.cpython-3**-darwin.so` exists. The Makefile generated by `aten/bindings/CMakeLists.txt` will produce `build/aten.cpython-3**-darwin.so`, which will immediately be moved by `setup.py` to `build/lib*/ember/aten.cpython-3**-darwin.so`. 
  4. The `setup()` function will copy this `.so` file to `ember/aten.cpython-3**-darwin.so`. The `.so` file must live within `ember`, the actual library, since `ember/__init__.py` must access it within the same directory level (cannot be higher). 

### Making sure everything's correct 

  Run the script `./run_tests.sh all` (args `python` to run just python tests and `cpp` to run just C++ tests), which will 
  1. Run all C++ unit tests for `aten`, ensuring that all functions work correctly. 
  2. Run all Python unit tests for `ember`, ensuring that additional functions work correctly and that the C++ functions are binded correctly. The stub (`.pyi`) files for `aten` are located in `ember/aten`. 

## Repository Structure 

  I tried to model a lot of the structure from Pytorch and TinyGrad. Very briefly, 

  1. `aten` contains the header and source files for the C++ low-level tensor library, such as basic operations and an autograd engine. 
  2. `docs` contains detailed documentation about each function.  
  3. `ember` contains the actual library, supporting high level models, dataloaders, and samplers. 
  4. `examples` are example scripts.  
  5. `tests` are testing modules for the `ember` library. 
  6. `CMakeLists.txt` generates the Makefiles needed to compile this library. 
  7. `setup.py` allows you to pip install this as a package. 
  8. `run_tests.sh` which is the main test running script. 

  For a more detailed explanation, look [here](docs/structure.md). 

## Getting Started 

### Ember Tensors 

Tensors are multidimensional arrays that can be initialized in a number of ways. 
```
import ember 

a = ember.Tensor([2]) # scalar
b = ember.Tensor([1, 2, 3])  # vector 
c = ember.Tensor([[1, 2], [3, 4]]) # 2D vector 
d = ember.Tensor([[[1, 2]]]) # 3D vector
```
Gradient computations aren't supported for 3D vectors yet. Say that you have a series of elementary operations on tensors. 
```
a = ember.Tensor([2, -3]) 
h = a ** 2
b = ember.Tensor([3, 5])

c = b * h

d = ember.Tensor([10, 1])
e = c.dot(d) 

f = ember.Tensor([-2])

g = f * e
```

### Linear Regression 

To perform linear regression, use the `LinearRegression` model. 
```
import ember 

X = ember.Tensor.uniform([20, 5], 0, 10, has_grad=False) 
params = ember.Tensor.arange(1, 6, 1, has_grad=False).reshape([5, 1], inplace=True)
Y_truth = X @ params 
noise = ember.Tensor.gaussian([20, 1], 0, 1, has_grad=False)
Y = Y_truth + noise

model = ember.LinearRegression(5) 
a = 1e-4

for i in range(1000): 
  y = model.forward(X) 
  diff = Y - y 
  squared_diff = diff ** 2 
  loss = squared_diff.sum() 
  loss.backprop(False) 

  model.W = model.W - a * model.W.grad.reshape([5, 1], inplace=False)
  model.b = model.b - a * model.b.grad.reshape([20, 1], inplace=False)

  if i % 25 == 0: 
    print(f"LOSS = {loss[0].item()}")
    print(model.W.reshape([5], inplace=False))
    print(model.b.reshape([20], inplace=False))

  del y, diff, squared_diff, loss 
```

### Multilayer Perceptrons 

To instantiate a MLP, just call it from models. In here we make a 2-layer MLP with a dummy dataset. For now only SGD with batch size 1 is supported.  
```
import ember 

X = ember.Tensor.uniform([20, 15], 0, 10, has_grad=False) 
params = ember.Tensor.arange(1, 16, 1, has_grad=False).reshape([15, 1], inplace=True)
Y_truth = X @ params 
noise = ember.Tensor.gaussian([20, 1], 0, 1, has_grad=False)
Y = Y_truth + noise

model = ember.MultiLayerPerceptron(15, 10) 
a = 1e-4

for epoch in range(500):  
  loss = ember.ScalarTensor(0)
  for i in range(20): 
    x = X[i] # (1, 15)
    y = Y[i] # (1, 1)
    y_ = model.forward(x) 
    diff = y - y_
    squared_diff = diff ** 2 
    loss = squared_diff.sum() 
    loss.backprop(False) 

    model.W1 = model.W1 - a * model.W1.grad.reshape(model.W1.shape, inplace=False)
    model.b1 = model.b1 - a * model.b1.grad.reshape(model.b1.shape, inplace=False)
    model.W2 = model.W2 - a * model.W2.grad.reshape(model.W2.shape, inplace=False)
    model.b2 = model.b2 - a * model.b2.grad.reshape(model.b2.shape, inplace=False)

  if epoch % 25 == 0: 
    print(f"LOSS = {loss[0].item()}")
```
Its outputs over 1 minute. 
```
LOSS = 256733.64437981808
LOSS = 203239.08846901066
LOSS = 160223.4554735339
LOSS = 125704.33716141782
LOSS = 98074.96981384761
LOSS = 76026.19871949886
LOSS = 58491.92389906721
LOSS = 44604.493032865605
LOSS = 33658.23285350788
LOSS = 25079.638682869212
LOSS = 18403.01062298029
LOSS = 13250.54496118543
LOSS = 9316.069468116035
LOSS = 6351.758695807299
LOSS = 4157.286052245369
LOSS = 2570.96819208677
LOSS = 1462.5380952427417
LOSS = 727.2493587808174
LOSS = 281.0683664354656
LOSS = 56.75530418715159
```


### Autograd

The C++ backend computes a directed acyclic graph (DAG) representing the operations done to compute `g`. You can then run `g.backprop()` to compute the gradients by applying the chain rule. This constructs the DAG and returns a topological sorting of its nodes. The gradients themselves, which are technically Jacobian matrices, are updated, with each mapping `x -> y` constructing a gradient tensor on `x` with value `dy/dx`. The gradients can be either accumulated by setting `backprop(intermediate=False)` so that the chain rule is not applied yet, or we can set `=True` to apply the chain rule to calculate the derivative of the tensor we called backprop on w.r.t. the rest of the tensors. 

```
top_sort = g.backprop()
print(a.grad) # [[4.0, 0.0], [0.0, -6.0]]
print(h.grad) # [[3.0, 0.0], [0.0, 5.0]]
print(b.grad) # [[4.0, 0.0], [0.0, 9.0]]
print(c.grad) # [[10.0, 1.0]]
print(d.grad) # [[12.0, 45.0]]
print(e.grad) # [[-2.0]]
print(f.grad) # [[165.0]]
print(g.grad) # [[1.0]]
```


Finally, we can visualize this using the `networkx` package. 

![Alt text](docs/img/computational_graph.png)

### Datasets

### Models and Training

### Monte Carlo Samplers

## Contributing 

To implement a new functionality in the `aten` library, you must 
1. Add the class or function header in `aten/src/Tensor.h` 
2. Add the implementation in the correct file (or create a new one) in `aten./*Tensor/*.cpp`. Make sure to update `aten/bindings/CMakeLists.txt` if needed.
3. Add its pybindings (if a public function that will be used in `ember`) in `aten/bindings/*bindings.cpp`. Make sure to update `aten/bindings/CMakeLists.txt` if needed. 
4. Add relevant C++ tests in `aten/test/`.  
5. Not necessary, but it's good to test it out on a personal script for a sanity check.  
6. Add to the stub files in `ember/aten/*.pyi`. 
7. Add Python tests in `test/`. 
8. If everything passes, you can submit a pull request. 

